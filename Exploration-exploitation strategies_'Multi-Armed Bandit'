# -*- coding: utf-8 -*-
"""
Created on Fri Oct 18 09:21:55 2019
Multi-Armed Bandit
exploration-exploitation strategies
@author: 63159
"""
import gym_bandits
import gym
import numpy as np
import math
import random
env = gym.make("BanditTenArmedGaussian-v0")
num_rounds = 20000
# Count of number of times an arm was pulled
count = np.zeros(10)
# Sum of rewards of each arm
sum_rewards = np.zeros(10)
# Q value which is the average reward
Q = np.zeros(10)
alpha = np.ones(10)
beta = np.ones(10)
def softmax(tau):
    #tau:temperature factor high:all arms will be explored equally
    #when tau is low, high-rewarding arms will be chosen
    total=sum([math.exp(val/tau) for val in Q])
    probs=[math.exp(val/tau)/total for val in Q]
    
    threshold=random.random()
    cumulative_prob=0.0
    for i in range(len(probs)):
        cumulative_prob+=probs[i]
        if cumulative_prob>threshold:
            return i
    return np.argmax(probs)

def epsilon_greedy(epsilon):
    rand = np.random.random()
    if rand < epsilon:
        action = env.action_space.sample()
    else:
        action = np.argmax(Q)
    return action

def UCB(iters):
    '''
    The idea behind UCB is very simple:
    1. Select the action (arm) that has a high sum of average reward and upper confidence bound
    2. Pull the arm and receive a reward
    3. Update the arm's reward and confidence bound
    UCB calculate: sqrt(2*log(t)/N(a))
    N(a):the number of times the arm was pulled
    t:the total number of rounds
    '''
    ucb=np.zeros(10)
    #explore all the arms
    if iters<10:
        #avoid log(0)
        return iters 
    else:
        for arm in range(10):
            #cauculate UCB
            upper_bound=math.sqrt(2*math.log(sum(count))/count[arm])
            #add ucb to the Q value
            ucb[arm]=Q[arm]+upper_bound
            #return the arm which has maximum value
        return np.argmax(ucb)

def thompson_sampling(alpha,beta):
    samples = [np.random.beta(alpha[i]+1,beta[i]+1) for i in range(10)]
    return np.argmax(samples)

for i in range(num_rounds):
    # Select the arm using thompson sampling
    arm = thompson_sampling(alpha,beta)
    # Select the arm using epsilon_greedy
    #arm=epsilon_greedy(0.5)
    # Select the arm using softmax
    #arm=softmax(0.9)
    # Select the arm using UCB
    #arm = UCB(i)
    observation,reward,done,info=env.step(arm)
    # update the count of that arm
    count[arm]+=1
    # Sum the rewards obtained from the arm
    sum_rewards[arm]+=reward
    # calculate Q value which is the average rewards of the arm
    Q[arm] = sum_rewards[arm]/count[arm]
    #update thompson_sampling distribution
    if reward >0:
        alpha[arm] += 1
    # If it is a negative reward increment beta
    else:
        beta[arm] += 1
print('The optimal arm is {}'.format(np.argmax(Q)))
